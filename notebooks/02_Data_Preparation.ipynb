{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# STEP 2: DATA PREPARATION\n",
        "\n",
        "**Extrovert-Introvert Classification - Data Cleaning and Preprocessing**\n",
        "\n",
        "This notebook handles comprehensive data preparation including:\n",
        "- Loading processed data from Step 1\n",
        "- Handling missing values and outliers\n",
        "- Feature validation and cleaning\n",
        "- Data type conversions and normalization\n",
        "- Feature engineering for personality classification\n",
        "- Preparing final datasets for modeling\n",
        "\n",
        "**Key Objectives:**\n",
        "- Clean and validate behavioral and psychological data\n",
        "- Handle missing values using domain-appropriate strategies\n",
        "- Engineer new features based on personality psychology\n",
        "- Balance classes using appropriate techniques\n",
        "- Export cleaned datasets for modeling phase\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 1: LIBRARY IMPORT COMPLETED\n",
            "============================================================\n",
            "All required libraries loaded successfully\n",
            "Available preprocessing methods:\n",
            "- Missing value imputation (Simple, KNN)\n",
            "- Outlier detection and handling\n",
            "- Feature scaling and normalization\n",
            "- Class balancing (SMOTE, Random sampling)\n",
            "- Statistical analysis tools\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical analysis\n",
        "from scipy import stats\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Data preprocessing\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "\n",
        "# Class balancing\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Progress bars\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"viridis\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: LIBRARY IMPORT COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(\"All required libraries loaded successfully\")\n",
        "print(\"Available preprocessing methods:\")\n",
        "print(\"- Missing value imputation (Simple, KNN)\")\n",
        "print(\"- Outlier detection and handling\")\n",
        "print(\"- Feature scaling and normalization\")\n",
        "print(\"- Class balancing (SMOTE, Random sampling)\")\n",
        "print(\"- Statistical analysis tools\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Load Data from Step 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 2: DATA LOADING\n",
            "============================================================\n",
            "Loading processed data from: ..\\data\\processed\\raw_personality_data.csv\n",
            "  Successfully loaded with encoding: utf-8\n",
            "\n",
            "Dataset loaded successfully from processed data!\n",
            "Shape: (2900, 8)\n",
            "Columns: ['Time_spent_Alone', 'Stage_fear', 'Social_event_attendance', 'Going_outside', 'Drained_after_socializing', 'Friends_circle_size', 'Post_frequency', 'Personality']\n",
            "Memory usage: 0.55 MB\n",
            "\n",
            "First 3 rows:\n",
            "   Time_spent_Alone Stage_fear  Social_event_attendance  Going_outside  \\\n",
            "0               4.0         No                      4.0            6.0   \n",
            "1               9.0        Yes                      0.0            0.0   \n",
            "2               9.0        Yes                      1.0            2.0   \n",
            "\n",
            "  Drained_after_socializing  Friends_circle_size  Post_frequency Personality  \n",
            "0                        No                 13.0             5.0   Extrovert  \n",
            "1                       Yes                  0.0             3.0   Introvert  \n",
            "2                       Yes                  5.0             2.0   Introvert  \n"
          ]
        }
      ],
      "source": [
        "# Load data using robust encoding handling\n",
        "def load_csv_safe(file_path, encodings=['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']):\n",
        "    \"\"\"\n",
        "    Load CSV file with multiple encoding attempts to handle any text encoding issues.\n",
        "    \"\"\"\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"  Successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"  Failed with {encoding} encoding\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"  Error with encoding {encoding}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    raise Exception(f\"Could not load file with any encoding: {encodings}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: DATA LOADING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Try to load from processed data first, then fallback to raw data\n",
        "processed_path = Path('../data/processed/raw_personality_data.csv')\n",
        "raw_path = Path('../data/raw/personality_dataset.csv')\n",
        "\n",
        "if processed_path.exists():\n",
        "    print(f\"Loading processed data from: {processed_path}\")\n",
        "    df = load_csv_safe(processed_path)\n",
        "    data_source = \"processed\"\n",
        "elif raw_path.exists():\n",
        "    print(f\"Loading raw data from: {raw_path}\")\n",
        "    df = load_csv_safe(raw_path)\n",
        "    data_source = \"raw\"\n",
        "else:\n",
        "    raise FileNotFoundError(\"No data files found. Please run Step 1 first.\")\n",
        "\n",
        "print(f\"\\nDataset loaded successfully from {data_source} data!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "\n",
        "# Display basic info\n",
        "print(f\"\\nFirst 3 rows:\")\n",
        "print(df.head(3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Validation and Feature Identification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 3: DATA VALIDATION AND FEATURE IDENTIFICATION\n",
            "============================================================\n",
            "COLUMN IDENTIFICATION RESULTS:\n",
            "Target column: Personality\n",
            "Behavioral features (5): ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\n",
            "Psychological features (2): ['Stage_fear', 'Drained_after_socializing']\n",
            "\n",
            "DATA TYPE VALIDATION:\n",
            "Numerical columns (5): ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', 'Friends_circle_size', 'Post_frequency']\n",
            "Categorical columns (3): ['Stage_fear', 'Drained_after_socializing', 'Personality']\n",
            "\n",
            "DATA QUALITY OVERVIEW:\n",
            "Total rows: 2,900\n",
            "Total columns: 8\n",
            "Missing values: 458\n",
            "Duplicate rows: 388\n",
            "Memory usage: 0.55 MB\n",
            "\n",
            "TARGET VARIABLE ANALYSIS:\n",
            "  Extrovert: 1,491 (51.4%)\n",
            "  Introvert: 1,409 (48.6%)\n",
            "  Class balance ratio: 1.06:1\n",
            "  Balance assessment: WELL BALANCED\n",
            "\n",
            "Validation completed. Ready for data cleaning pipeline.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: DATA VALIDATION AND FEATURE IDENTIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Auto-detect key columns based on personality dataset structure\n",
        "target_column = None\n",
        "behavioral_features = []\n",
        "psychological_features = []\n",
        "\n",
        "# Identify target column\n",
        "for col in df.columns:\n",
        "    if col.upper() in ['PERSONALITY', 'TARGET', 'LABEL', 'CLASS']:\n",
        "        target_column = col\n",
        "        break\n",
        "\n",
        "# Identify behavioral features (numerical scales)\n",
        "expected_behavioral = ['Time_spent_Alone', 'Social_event_attendance', 'Going_outside', \n",
        "                      'Friends_circle_size', 'Post_frequency']\n",
        "for feature in expected_behavioral:\n",
        "    if feature in df.columns:\n",
        "        behavioral_features.append(feature)\n",
        "\n",
        "# Identify psychological features (categorical)\n",
        "expected_psychological = ['Stage_fear', 'Drained_after_socializing']\n",
        "for feature in expected_psychological:\n",
        "    if feature in df.columns:\n",
        "        psychological_features.append(feature)\n",
        "\n",
        "print(f\"COLUMN IDENTIFICATION RESULTS:\")\n",
        "print(f\"Target column: {target_column}\")\n",
        "print(f\"Behavioral features ({len(behavioral_features)}): {behavioral_features}\")\n",
        "print(f\"Psychological features ({len(psychological_features)}): {psychological_features}\")\n",
        "\n",
        "# Validate required columns exist\n",
        "if not target_column:\n",
        "    print(\"ERROR: Target column 'Personality' not found!\")\n",
        "    \n",
        "if len(behavioral_features) == 0:\n",
        "    print(\"ERROR: No behavioral features found!\")\n",
        "    \n",
        "if len(psychological_features) == 0:\n",
        "    print(\"ERROR: No psychological features found!\")\n",
        "\n",
        "# Check data types\n",
        "print(f\"\\nDATA TYPE VALIDATION:\")\n",
        "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "print(f\"Numerical columns ({len(numerical_cols)}): {numerical_cols}\")\n",
        "print(f\"Categorical columns ({len(categorical_cols)}): {categorical_cols}\")\n",
        "\n",
        "# Basic data quality checks\n",
        "print(f\"\\nDATA QUALITY OVERVIEW:\")\n",
        "print(f\"Total rows: {len(df):,}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
        "\n",
        "# Memory usage\n",
        "memory_mb = df.memory_usage(deep=True).sum() / 1024 / 1024\n",
        "print(f\"Memory usage: {memory_mb:.2f} MB\")\n",
        "\n",
        "if target_column and target_column in df.columns:\n",
        "    print(f\"\\nTARGET VARIABLE ANALYSIS:\")\n",
        "    target_counts = df[target_column].value_counts()\n",
        "    for personality, count in target_counts.items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"  {personality}: {count:,} ({pct:.1f}%)\")\n",
        "    \n",
        "    balance_ratio = target_counts.max() / target_counts.min()\n",
        "    print(f\"  Class balance ratio: {balance_ratio:.2f}:1\")\n",
        "    \n",
        "    if balance_ratio <= 1.5:\n",
        "        balance_status = \"WELL BALANCED\"\n",
        "    elif balance_ratio <= 3.0:\n",
        "        balance_status = \"MODERATELY IMBALANCED\"\n",
        "    else:\n",
        "        balance_status = \"HIGHLY IMBALANCED\"\n",
        "    \n",
        "    print(f\"  Balance assessment: {balance_status}\")\n",
        "\n",
        "print(f\"\\nValidation completed. Ready for data cleaning pipeline.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Data Cleaning and Missing Value Handling Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 4: DEFINING DATA CLEANING FUNCTIONS\n",
            "============================================================\n",
            "Data cleaning functions defined successfully:\n",
            "- handle_missing_values(): Domain-specific missing value imputation\n",
            "- detect_outliers(): IQR and Z-score outlier detection\n",
            "- validate_feature_ranges(): Validate 0-10 behavioral scales\n",
            "- clean_categorical_features(): Standardize categorical values\n",
            "\n",
            "Ready to apply data cleaning pipeline...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: DEFINING DATA CLEANING FUNCTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def handle_missing_values(df, strategy='domain_specific'):\n",
        "    \"\"\"\n",
        "    Handle missing values using domain-specific strategies for personality data.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    if strategy == 'domain_specific':\n",
        "        # For behavioral features (0-10 scales), use median imputation\n",
        "        for feature in behavioral_features:\n",
        "            if feature in df_clean.columns and df_clean[feature].isnull().any():\n",
        "                median_val = df_clean[feature].median()\n",
        "                df_clean[feature].fillna(median_val, inplace=True)\n",
        "                print(f\"  {feature}: Filled {df[feature].isnull().sum()} missing values with median ({median_val})\")\n",
        "        \n",
        "        # For categorical psychological features, use mode imputation\n",
        "        for feature in psychological_features:\n",
        "            if feature in df_clean.columns and df_clean[feature].isnull().any():\n",
        "                mode_val = df_clean[feature].mode().iloc[0] if not df_clean[feature].mode().empty else 'Unknown'\n",
        "                df_clean[feature].fillna(mode_val, inplace=True)\n",
        "                print(f\"  {feature}: Filled {df[feature].isnull().sum()} missing values with mode ({mode_val})\")\n",
        "        \n",
        "        # For target variable, drop rows with missing values\n",
        "        if target_column and df_clean[target_column].isnull().any():\n",
        "            before_count = len(df_clean)\n",
        "            df_clean = df_clean.dropna(subset=[target_column])\n",
        "            dropped_count = before_count - len(df_clean)\n",
        "            print(f\"  {target_column}: Removed {dropped_count} rows with missing target values\")\n",
        "            \n",
        "    return df_clean\n",
        "\n",
        "def detect_outliers(df, features, method='iqr', threshold=1.5):\n",
        "    \"\"\"\n",
        "    Detect outliers in numerical features using IQR or Z-score methods.\n",
        "    \"\"\"\n",
        "    outlier_info = {}\n",
        "    \n",
        "    for feature in features:\n",
        "        if feature in df.columns:\n",
        "            if method == 'iqr':\n",
        "                Q1 = df[feature].quantile(0.25)\n",
        "                Q3 = df[feature].quantile(0.75)\n",
        "                IQR = Q3 - Q1\n",
        "                lower_bound = Q1 - threshold * IQR\n",
        "                upper_bound = Q3 + threshold * IQR\n",
        "                \n",
        "                outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
        "                outlier_count = len(outliers)\n",
        "                \n",
        "            elif method == 'zscore':\n",
        "                z_scores = np.abs(zscore(df[feature]))\n",
        "                outliers = df[z_scores > threshold]\n",
        "                outlier_count = len(outliers)\n",
        "                \n",
        "            outlier_info[feature] = {\n",
        "                'count': outlier_count,\n",
        "                'percentage': (outlier_count / len(df)) * 100\n",
        "            }\n",
        "    \n",
        "    return outlier_info\n",
        "\n",
        "def validate_feature_ranges(df, features):\n",
        "    \"\"\"\n",
        "    Validate that behavioral features are within expected ranges (0-10 scale).\n",
        "    \"\"\"\n",
        "    validation_results = {}\n",
        "    \n",
        "    for feature in features:\n",
        "        if feature in df.columns:\n",
        "            min_val = df[feature].min()\n",
        "            max_val = df[feature].max()\n",
        "            \n",
        "            # Check if values are within 0-10 range\n",
        "            within_range = (min_val >= 0) and (max_val <= 10)\n",
        "            out_of_range_count = len(df[(df[feature] < 0) | (df[feature] > 10)])\n",
        "            \n",
        "            validation_results[feature] = {\n",
        "                'min': min_val,\n",
        "                'max': max_val,\n",
        "                'within_expected_range': within_range,\n",
        "                'out_of_range_count': out_of_range_count\n",
        "            }\n",
        "    \n",
        "    return validation_results\n",
        "\n",
        "def clean_categorical_features(df, features):\n",
        "    \"\"\"\n",
        "    Clean and standardize categorical features.\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    \n",
        "    for feature in features:\n",
        "        if feature in df_clean.columns:\n",
        "            # Convert to string and strip whitespace\n",
        "            df_clean[feature] = df_clean[feature].astype(str).str.strip()\n",
        "            \n",
        "            # Standardize common variations\n",
        "            if feature == 'Stage_fear':\n",
        "                df_clean[feature] = df_clean[feature].replace({\n",
        "                    'yes': 'Yes', 'YES': 'Yes', 'y': 'Yes', 'Y': 'Yes',\n",
        "                    'no': 'No', 'NO': 'No', 'n': 'No', 'N': 'No'\n",
        "                })\n",
        "            \n",
        "            if feature == 'Drained_after_socializing':\n",
        "                df_clean[feature] = df_clean[feature].replace({\n",
        "                    'yes': 'Yes', 'YES': 'Yes', 'y': 'Yes', 'Y': 'Yes',\n",
        "                    'no': 'No', 'NO': 'No', 'n': 'No', 'N': 'No'\n",
        "                })\n",
        "            \n",
        "            # Remove 'nan' strings\n",
        "            df_clean[feature] = df_clean[feature].replace('nan', np.nan)\n",
        "    \n",
        "    return df_clean\n",
        "\n",
        "print(\"Data cleaning functions defined successfully:\")\n",
        "print(\"- handle_missing_values(): Domain-specific missing value imputation\")\n",
        "print(\"- detect_outliers(): IQR and Z-score outlier detection\")\n",
        "print(\"- validate_feature_ranges(): Validate 0-10 behavioral scales\")\n",
        "print(\"- clean_categorical_features(): Standardize categorical values\")\n",
        "print(\"\\nReady to apply data cleaning pipeline...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Apply Data Cleaning Pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 5: APPLYING DATA CLEANING PIPELINE\n",
            "============================================================\n",
            "Original dataset: 2,900 rows, 8 columns\n",
            "Original missing values: 458\n",
            "\n",
            "Step 5.1: Removed 388 duplicate rows\n",
            "\n",
            "Step 5.2: Cleaning categorical features...\n",
            "\n",
            "Step 5.3: Validating behavioral feature ranges...\n",
            "  Time_spent_Alone: Range [0.0, 11.0], Valid range: False, Out of range: 116\n",
            "  Social_event_attendance: Range [0.0, 10.0], Valid range: True, Out of range: 0\n",
            "  Going_outside: Range [0.0, 7.0], Valid range: True, Out of range: 0\n",
            "  Friends_circle_size: Range [0.0, 15.0], Valid range: False, Out of range: 571\n",
            "  Post_frequency: Range [0.0, 10.0], Valid range: True, Out of range: 0\n",
            "  Time_spent_Alone: Capped 116 out-of-range values to [0, 10]\n",
            "  Friends_circle_size: Capped 571 out-of-range values to [0, 10]\n",
            "\n",
            "Step 5.5: Detecting outliers in behavioral features...\n",
            "  Time_spent_Alone: 0 outliers (0.0%)\n",
            "  Social_event_attendance: 0 outliers (0.0%)\n",
            "  Going_outside: 0 outliers (0.0%)\n",
            "  Friends_circle_size: 0 outliers (0.0%)\n",
            "  Post_frequency: 0 outliers (0.0%)\n",
            "\n",
            "Step 5.6: Handling missing values...\n",
            "Missing values before cleaning: 449\n",
            "  Time_spent_Alone: Filled 61 missing values with median (3.0)\n",
            "  Social_event_attendance: Filled 61 missing values with median (4.0)\n",
            "  Going_outside: Filled 65 missing values with median (3.0)\n",
            "  Friends_circle_size: Filled 75 missing values with median (6.0)\n",
            "  Post_frequency: Filled 63 missing values with median (3.0)\n",
            "  Stage_fear: Filled 73 missing values with mode (No)\n",
            "  Drained_after_socializing: Filled 51 missing values with mode (No)\n",
            "Missing values after cleaning: 0\n",
            "\n",
            "Step 5.7: Final data validation...\n",
            "Final dataset: 2,512 rows, 8 columns\n",
            "Final missing values: 0\n",
            "Rows retained: 86.6%\n",
            "\n",
            "Sample of cleaned data:\n",
            "  Personality  Time_spent_Alone  Social_event_attendance  Going_outside  \\\n",
            "0   Extrovert               4.0                      4.0            6.0   \n",
            "1   Introvert               9.0                      0.0            0.0   \n",
            "2   Introvert               9.0                      1.0            2.0   \n",
            "\n",
            "  Stage_fear Drained_after_socializing  \n",
            "0         No                        No  \n",
            "1        Yes                       Yes  \n",
            "2        Yes                       Yes  \n",
            "\n",
            "Data cleaning pipeline completed successfully!\n"
          ]
        }
      ],
      "source": [
        "if target_column and (behavioral_features or psychological_features):\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 5: APPLYING DATA CLEANING PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Store original data info\n",
        "    original_shape = df.shape\n",
        "    original_missing = df.isnull().sum().sum()\n",
        "    \n",
        "    print(f\"Original dataset: {original_shape[0]:,} rows, {original_shape[1]} columns\")\n",
        "    print(f\"Original missing values: {original_missing}\")\n",
        "    \n",
        "    # Step 5.1: Remove duplicates\n",
        "    df_before_dup = df.copy()\n",
        "    df = df.drop_duplicates()\n",
        "    duplicates_removed = len(df_before_dup) - len(df)\n",
        "    if duplicates_removed > 0:\n",
        "        print(f\"\\nStep 5.1: Removed {duplicates_removed} duplicate rows\")\n",
        "    else:\n",
        "        print(f\"\\nStep 5.1: No duplicate rows found\")\n",
        "    \n",
        "    # Step 5.2: Clean categorical features\n",
        "    print(f\"\\nStep 5.2: Cleaning categorical features...\")\n",
        "    df = clean_categorical_features(df, psychological_features)\n",
        "    \n",
        "    # Step 5.3: Validate feature ranges\n",
        "    print(f\"\\nStep 5.3: Validating behavioral feature ranges...\")\n",
        "    validation_results = validate_feature_ranges(df, behavioral_features)\n",
        "    \n",
        "    for feature, results in validation_results.items():\n",
        "        print(f\"  {feature}: Range [{results['min']:.1f}, {results['max']:.1f}], \"\n",
        "              f\"Valid range: {results['within_expected_range']}, \"\n",
        "              f\"Out of range: {results['out_of_range_count']}\")\n",
        "    \n",
        "    # Step 5.4: Handle out-of-range values by capping\n",
        "    for feature in behavioral_features:\n",
        "        if feature in df.columns:\n",
        "            original_out_of_range = len(df[(df[feature] < 0) | (df[feature] > 10)])\n",
        "            df[feature] = df[feature].clip(0, 10)\n",
        "            \n",
        "            if original_out_of_range > 0:\n",
        "                print(f\"  {feature}: Capped {original_out_of_range} out-of-range values to [0, 10]\")\n",
        "    \n",
        "    # Step 5.5: Detect outliers (before missing value handling)\n",
        "    print(f\"\\nStep 5.5: Detecting outliers in behavioral features...\")\n",
        "    outlier_info = detect_outliers(df, behavioral_features, method='iqr', threshold=1.5)\n",
        "    \n",
        "    for feature, info in outlier_info.items():\n",
        "        print(f\"  {feature}: {info['count']} outliers ({info['percentage']:.1f}%)\")\n",
        "    \n",
        "    # Step 5.6: Handle missing values\n",
        "    print(f\"\\nStep 5.6: Handling missing values...\")\n",
        "    missing_before = df.isnull().sum().sum()\n",
        "    print(f\"Missing values before cleaning: {missing_before}\")\n",
        "    \n",
        "    if missing_before > 0:\n",
        "        df = handle_missing_values(df, strategy='domain_specific')\n",
        "        missing_after = df.isnull().sum().sum()\n",
        "        print(f\"Missing values after cleaning: {missing_after}\")\n",
        "    else:\n",
        "        print(\"No missing values to handle\")\n",
        "    \n",
        "    # Step 5.7: Final validation\n",
        "    print(f\"\\nStep 5.7: Final data validation...\")\n",
        "    final_shape = df.shape\n",
        "    final_missing = df.isnull().sum().sum()\n",
        "    \n",
        "    print(f\"Final dataset: {final_shape[0]:,} rows, {final_shape[1]} columns\")\n",
        "    print(f\"Final missing values: {final_missing}\")\n",
        "    print(f\"Rows retained: {(final_shape[0]/original_shape[0])*100:.1f}%\")\n",
        "    \n",
        "    # Display cleaned data sample\n",
        "    print(f\"\\nSample of cleaned data:\")\n",
        "    sample_cols = [target_column] + behavioral_features[:3] + psychological_features[:2]\n",
        "    available_cols = [col for col in sample_cols if col in df.columns]\n",
        "    print(df[available_cols].head(3))\n",
        "    \n",
        "    print(f\"\\nData cleaning pipeline completed successfully!\")\n",
        "    \n",
        "else:\n",
        "    print(\"ERROR: Cannot proceed without target column and feature columns\")\n",
        "    print(f\"Target column found: {target_column is not None}\")\n",
        "    print(f\"Behavioral features found: {len(behavioral_features)}\")\n",
        "    print(f\"Psychological features found: {len(psychological_features)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Feature Engineering for Personality Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 6: FEATURE ENGINEERING FOR PERSONALITY CLASSIFICATION\n",
            "============================================================\n",
            "Creating personality-specific engineered features...\n",
            "✓ Social_Activity_Score: Average of social event attendance, going outside, and posting frequency\n",
            "✓ Introversion_Score: Average of time alone, stage fear, and social draining\n",
            "✓ Social_Comfort: Combination of friend circle size and absence of stage fear\n",
            "✓ Digital_vs_Physical_Social: Difference between online and offline social engagement\n",
            "✓ Social_Energy_Balance: Social attendance adjusted for energy drain\n",
            "✓ Binary encodings: Has_Stage_Fear, Gets_Drained_Socializing, Is_Introvert\n",
            "\n",
            "Feature engineering summary:\n",
            "Original behavioral features: 5\n",
            "Engineered composite features: 5\n",
            "Binary encoded features: 3\n",
            "Total features for modeling: 13\n",
            "\n",
            "Engineered feature statistics:\n",
            "  Social_Activity_Score: Mean=3.74, Std=2.41\n",
            "  Introversion_Score: Mean=4.27, Std=4.25\n",
            "  Social_Comfort: Mean=5.81, Std=4.01\n",
            "  Digital_vs_Physical_Social: Mean=-0.40, Std=2.21\n",
            "  Social_Energy_Balance: Mean=2.05, Std=5.11\n",
            "\n",
            "Feature engineering completed successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 6: FEATURE ENGINEERING FOR PERSONALITY CLASSIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Create engineered features based on personality psychology\n",
        "print(\"Creating personality-specific engineered features...\")\n",
        "\n",
        "# 6.1: Social Activity Composite Score\n",
        "if all(feature in df.columns for feature in ['Social_event_attendance', 'Going_outside', 'Post_frequency']):\n",
        "    df['Social_Activity_Score'] = (df['Social_event_attendance'] + df['Going_outside'] + df['Post_frequency']) / 3\n",
        "    print(f\"✓ Social_Activity_Score: Average of social event attendance, going outside, and posting frequency\")\n",
        "\n",
        "# 6.2: Introversion Tendency Score\n",
        "if all(feature in df.columns for feature in ['Time_spent_Alone', 'Stage_fear', 'Drained_after_socializing']):\n",
        "    # Convert categorical to numerical for calculation\n",
        "    stage_fear_numeric = df['Stage_fear'].map({'Yes': 10, 'No': 0}) if 'Stage_fear' in df.columns else 0\n",
        "    drained_numeric = df['Drained_after_socializing'].map({'Yes': 10, 'No': 0}) if 'Drained_after_socializing' in df.columns else 0\n",
        "    \n",
        "    df['Introversion_Score'] = (df['Time_spent_Alone'] + stage_fear_numeric + drained_numeric) / 3\n",
        "    print(f\"✓ Introversion_Score: Average of time alone, stage fear, and social draining\")\n",
        "\n",
        "# 6.3: Social Comfort Level\n",
        "if 'Friends_circle_size' in df.columns and 'Stage_fear' in df.columns:\n",
        "    stage_fear_inverted = df['Stage_fear'].map({'Yes': 0, 'No': 10})\n",
        "    df['Social_Comfort'] = (df['Friends_circle_size'] + stage_fear_inverted) / 2\n",
        "    print(f\"✓ Social_Comfort: Combination of friend circle size and absence of stage fear\")\n",
        "\n",
        "# 6.4: Digital vs Physical Social Engagement\n",
        "if 'Post_frequency' in df.columns and 'Social_event_attendance' in df.columns:\n",
        "    df['Digital_vs_Physical_Social'] = df['Post_frequency'] - df['Social_event_attendance']\n",
        "    print(f\"✓ Digital_vs_Physical_Social: Difference between online and offline social engagement\")\n",
        "\n",
        "# 6.5: Energy Drain from Social Interaction\n",
        "if 'Drained_after_socializing' in df.columns and 'Social_event_attendance' in df.columns:\n",
        "    drained_penalty = df['Drained_after_socializing'].map({'Yes': -5, 'No': 0})\n",
        "    df['Social_Energy_Balance'] = df['Social_event_attendance'] + drained_penalty\n",
        "    print(f\"✓ Social_Energy_Balance: Social attendance adjusted for energy drain\")\n",
        "\n",
        "# 6.6: Binary feature encodings for modeling\n",
        "binary_features_created = []\n",
        "\n",
        "if 'Stage_fear' in df.columns:\n",
        "    df['Has_Stage_Fear'] = (df['Stage_fear'] == 'Yes').astype(int)\n",
        "    binary_features_created.append('Has_Stage_Fear')\n",
        "\n",
        "if 'Drained_after_socializing' in df.columns:\n",
        "    df['Gets_Drained_Socializing'] = (df['Drained_after_socializing'] == 'Yes').astype(int)\n",
        "    binary_features_created.append('Gets_Drained_Socializing')\n",
        "\n",
        "if target_column in df.columns:\n",
        "    df['Is_Introvert'] = (df[target_column] == 'Introvert').astype(int)\n",
        "    binary_features_created.append('Is_Introvert')\n",
        "\n",
        "print(f\"✓ Binary encodings: {', '.join(binary_features_created)}\")\n",
        "\n",
        "# 6.7: Feature scaling groups\n",
        "behavioral_numeric_features = [col for col in behavioral_features if col in df.columns]\n",
        "engineered_features = [col for col in df.columns if col in [\n",
        "    'Social_Activity_Score', 'Introversion_Score', 'Social_Comfort',\n",
        "    'Digital_vs_Physical_Social', 'Social_Energy_Balance'\n",
        "]]\n",
        "\n",
        "print(f\"\\nFeature engineering summary:\")\n",
        "print(f\"Original behavioral features: {len(behavioral_numeric_features)}\")\n",
        "print(f\"Engineered composite features: {len(engineered_features)}\")\n",
        "print(f\"Binary encoded features: {len(binary_features_created)}\")\n",
        "print(f\"Total features for modeling: {len(behavioral_numeric_features) + len(engineered_features) + len(binary_features_created)}\")\n",
        "\n",
        "# Display engineered feature statistics\n",
        "if engineered_features:\n",
        "    print(f\"\\nEngineered feature statistics:\")\n",
        "    for feature in engineered_features:\n",
        "        if feature in df.columns:\n",
        "            mean_val = df[feature].mean()\n",
        "            std_val = df[feature].std()\n",
        "            print(f\"  {feature}: Mean={mean_val:.2f}, Std={std_val:.2f}\")\n",
        "\n",
        "print(f\"\\nFeature engineering completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Class Balancing Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 7: CLASS BALANCING ANALYSIS\n",
            "============================================================\n",
            "Current class distribution:\n",
            "  Extrovert: 1,417 samples (56.4%)\n",
            "  Introvert: 1,095 samples (43.6%)\n",
            "\n",
            "Class imbalance ratio: 1.29:1\n",
            "Balance assessment: WELL BALANCED\n",
            "Dataset is well balanced. No balancing techniques applied.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 7: CLASS BALANCING ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if target_column and target_column in df.columns:\n",
        "    # Analyze current class distribution\n",
        "    class_counts = df[target_column].value_counts()\n",
        "    total_samples = len(df)\n",
        "    \n",
        "    print(f\"Current class distribution:\")\n",
        "    for personality_type, count in class_counts.items():\n",
        "        percentage = (count / total_samples) * 100\n",
        "        print(f\"  {personality_type}: {count:,} samples ({percentage:.1f}%)\")\n",
        "    \n",
        "    # Calculate imbalance ratio\n",
        "    max_class = class_counts.max()\n",
        "    min_class = class_counts.min()\n",
        "    imbalance_ratio = max_class / min_class\n",
        "    \n",
        "    print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "    \n",
        "    # Determine if balancing is needed\n",
        "    if imbalance_ratio <= 1.5:\n",
        "        balance_status = \"WELL BALANCED\"\n",
        "        needs_balancing = False\n",
        "    elif imbalance_ratio <= 3.0:\n",
        "        balance_status = \"MODERATELY IMBALANCED\"\n",
        "        needs_balancing = True\n",
        "    else:\n",
        "        balance_status = \"HIGHLY IMBALANCED\"\n",
        "        needs_balancing = True\n",
        "    \n",
        "    print(f\"Balance assessment: {balance_status}\")\n",
        "    \n",
        "    if needs_balancing:\n",
        "        print(f\"\\nApplying class balancing techniques...\")\n",
        "        \n",
        "        # Prepare features for balancing (exclude target and non-predictive columns)\n",
        "        feature_columns = behavioral_numeric_features + engineered_features + binary_features_created\n",
        "        feature_columns = [col for col in feature_columns if col in df.columns and col != target_column]\n",
        "        \n",
        "        X = df[feature_columns]\n",
        "        y = df[target_column]\n",
        "        \n",
        "        print(f\"Using {len(feature_columns)} features for balancing: {feature_columns[:5]}{'...' if len(feature_columns) > 5 else ''}\")\n",
        "        \n",
        "        # Method 1: Random Undersampling\n",
        "        try:\n",
        "            undersampler = RandomUnderSampler(random_state=42)\n",
        "            X_under, y_under = undersampler.fit_resample(X, y)\n",
        "            \n",
        "            # Reconstruct dataframe\n",
        "            df_undersampled = pd.DataFrame(X_under, columns=feature_columns)\n",
        "            df_undersampled[target_column] = y_under\n",
        "            \n",
        "            # Add back any other columns not used in balancing\n",
        "            other_cols = [col for col in df.columns if col not in feature_columns and col != target_column]\n",
        "            for col in other_cols:\n",
        "                if col in df.columns:\n",
        "                    # Sample the other columns based on the undersampled indices\n",
        "                    df_undersampled[col] = df[col].iloc[undersampler.sample_indices_].reset_index(drop=True)\n",
        "            \n",
        "            print(f\"  Undersampling: {len(df)} → {len(df_undersampled)} samples\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Undersampling failed: {e}\")\n",
        "            df_undersampled = df.copy()\n",
        "        \n",
        "        # Method 2: Random Oversampling\n",
        "        try:\n",
        "            oversampler = RandomOverSampler(random_state=42)\n",
        "            X_over, y_over = oversampler.fit_resample(X, y)\n",
        "            \n",
        "            # Reconstruct dataframe\n",
        "            df_oversampled = pd.DataFrame(X_over, columns=feature_columns)\n",
        "            df_oversampled[target_column] = y_over\n",
        "            \n",
        "            # For oversampled data, we need to handle the additional columns differently\n",
        "            # since we have more rows than original data\n",
        "            other_cols = [col for col in df.columns if col not in feature_columns and col != target_column]\n",
        "            for col in other_cols:\n",
        "                if col in df.columns:\n",
        "                    # Replicate other columns based on oversampling pattern\n",
        "                    original_values = df[col].values\n",
        "                    oversampled_values = original_values[oversampler.sample_indices_]\n",
        "                    df_oversampled[col] = oversampled_values\n",
        "            \n",
        "            print(f\"  Oversampling: {len(df)} → {len(df_oversampled)} samples\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"  Oversampling failed: {e}\")\n",
        "            df_oversampled = df.copy()\n",
        "        \n",
        "        # Method 3: SMOTE (if possible with enough samples)\n",
        "        try:\n",
        "            if len(df) >= 12:  # SMOTE needs at least 6 samples per class typically\n",
        "                smote = SMOTE(random_state=42, k_neighbors=min(5, min_class-1))\n",
        "                X_smote, y_smote = smote.fit_resample(X, y)\n",
        "                \n",
        "                # Reconstruct dataframe\n",
        "                df_smote = pd.DataFrame(X_smote, columns=feature_columns)\n",
        "                df_smote[target_column] = y_smote\n",
        "                \n",
        "                print(f\"  SMOTE: {len(df)} → {len(df_smote)} samples\")\n",
        "            else:\n",
        "                df_smote = df_oversampled.copy()\n",
        "                print(f\"  SMOTE skipped: Dataset too small\")\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  SMOTE failed: {e}\")\n",
        "            df_smote = df_oversampled.copy()\n",
        "        \n",
        "        # Verify balanced distributions\n",
        "        print(f\"\\nBalanced datasets created:\")\n",
        "        print(f\"- Original dataset: {df.shape[0]:,} samples\")\n",
        "        if 'df_undersampled' in locals():\n",
        "            under_dist = df_undersampled[target_column].value_counts()\n",
        "            print(f\"- Undersampled dataset: {df_undersampled.shape[0]:,} samples, \"\n",
        "                  f\"balance: {under_dist.iloc[0]}:{under_dist.iloc[1]}\")\n",
        "        if 'df_oversampled' in locals():\n",
        "            over_dist = df_oversampled[target_column].value_counts()\n",
        "            print(f\"- Oversampled dataset: {df_oversampled.shape[0]:,} samples, \"\n",
        "                  f\"balance: {over_dist.iloc[0]}:{over_dist.iloc[1]}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"Dataset is well balanced. No balancing techniques applied.\")\n",
        "        df_undersampled = df.copy()\n",
        "        df_oversampled = df.copy()\n",
        "        if 'df_smote' not in locals():\n",
        "            df_smote = df.copy()\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Cannot proceed without target column\")\n",
        "    print(f\"Target column found: {target_column}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 8. Data Export and Final Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 8: DATA EXPORT AND FINAL PROCESSING\n",
            "============================================================\n",
            "Using main project data directory: ../data/processed\n",
            "Saved cleaned dataset to: ../data/processed\\personality_dataset_cleaned.csv\n",
            "  Shape: (2512, 16)\n",
            "Saved feature metadata to: ../data/processed\\feature_metadata.csv\n",
            "Saved processing summary to: ../data/processed\\data_preparation_summary.json\n",
            "\n",
            "DATA PREPARATION SUMMARY:\n",
            "Target column: 'Personality'\n",
            "Behavioral features: 5\n",
            "Engineered features: 5\n",
            "Binary features: 3\n",
            "Total modeling features: 13\n",
            "\n",
            "Datasets exported:\n",
            "- personality_dataset_cleaned.csv: Main processed dataset\n",
            "- feature_metadata.csv: Feature information for modeling\n",
            "- data_preparation_summary.json: Processing metadata\n"
          ]
        }
      ],
      "source": [
        "# Export cleaned datasets to the main data directory\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 8: DATA EXPORT AND FINAL PROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Always use the main project data directory (never create local ones)\n",
        "processed_dir = \"../data/processed\"\n",
        "\n",
        "# Verify the main data directory exists\n",
        "if os.path.exists(processed_dir):\n",
        "    print(f\"Using main project data directory: {processed_dir}\")\n",
        "    \n",
        "    # Export original cleaned dataset\n",
        "    if target_column and target_column in df.columns:\n",
        "        output_path = os.path.join(processed_dir, \"personality_dataset_cleaned.csv\")\n",
        "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "        print(f\"Saved cleaned dataset to: {output_path}\")\n",
        "        print(f\"  Shape: {df.shape}\")\n",
        "        \n",
        "        # Export undersampled dataset if available\n",
        "        if 'df_undersampled' in locals() and len(df_undersampled) != len(df):\n",
        "            output_path = os.path.join(processed_dir, \"personality_dataset_undersampled.csv\")\n",
        "            df_undersampled.to_csv(output_path, index=False, encoding='utf-8')\n",
        "            print(f\"Saved undersampled dataset to: {output_path}\")\n",
        "            print(f\"  Shape: {df_undersampled.shape}\")\n",
        "        \n",
        "        # Export oversampled dataset if available\n",
        "        if 'df_oversampled' in locals() and len(df_oversampled) != len(df):\n",
        "            output_path = os.path.join(processed_dir, \"personality_dataset_oversampled.csv\")\n",
        "            df_oversampled.to_csv(output_path, index=False, encoding='utf-8')\n",
        "            print(f\"Saved oversampled dataset to: {output_path}\")\n",
        "            print(f\"  Shape: {df_oversampled.shape}\")\n",
        "        \n",
        "        # Export SMOTE dataset if available\n",
        "        if 'df_smote' in locals() and len(df_smote) != len(df):\n",
        "            output_path = os.path.join(processed_dir, \"personality_dataset_smote.csv\")\n",
        "            df_smote.to_csv(output_path, index=False, encoding='utf-8')\n",
        "            print(f\"Saved SMOTE dataset to: {output_path}\")\n",
        "            print(f\"  Shape: {df_smote.shape}\")\n",
        "        \n",
        "        # Create feature metadata for modeling\n",
        "        all_features = behavioral_numeric_features + engineered_features + binary_features_created\n",
        "        feature_metadata = []\n",
        "        \n",
        "        for feature in all_features:\n",
        "            if feature in df.columns:\n",
        "                feature_info = {\n",
        "                    'feature_name': feature,\n",
        "                    'feature_type': 'behavioral' if feature in behavioral_numeric_features else \n",
        "                                  'engineered' if feature in engineered_features else 'binary',\n",
        "                    'data_type': str(df[feature].dtype),\n",
        "                    'min_value': float(df[feature].min()) if pd.api.types.is_numeric_dtype(df[feature]) else None,\n",
        "                    'max_value': float(df[feature].max()) if pd.api.types.is_numeric_dtype(df[feature]) else None,\n",
        "                    'mean_value': float(df[feature].mean()) if pd.api.types.is_numeric_dtype(df[feature]) else None,\n",
        "                    'std_value': float(df[feature].std()) if pd.api.types.is_numeric_dtype(df[feature]) else None,\n",
        "                    'missing_values': int(df[feature].isnull().sum()),\n",
        "                    'unique_values': int(df[feature].nunique())\n",
        "                }\n",
        "                feature_metadata.append(feature_info)\n",
        "        \n",
        "        # Save feature metadata\n",
        "        feature_metadata_df = pd.DataFrame(feature_metadata)\n",
        "        metadata_path = os.path.join(processed_dir, \"feature_metadata.csv\")\n",
        "        feature_metadata_df.to_csv(metadata_path, index=False, encoding='utf-8')\n",
        "        print(f\"Saved feature metadata to: {metadata_path}\")\n",
        "        \n",
        "        # Create processing summary\n",
        "        processing_summary = {\n",
        "            'original_dataset_path': str(processed_path if processed_path.exists() else raw_path),\n",
        "            'target_column': str(target_column) if target_column else None,\n",
        "            'behavioral_features': [str(f) for f in behavioral_numeric_features],\n",
        "            'psychological_features': [str(f) for f in psychological_features],\n",
        "            'engineered_features': [str(f) for f in engineered_features],\n",
        "            'binary_features': [str(f) for f in binary_features_created],\n",
        "            'original_samples': int(len(df)),\n",
        "            'final_samples': int(len(df)),\n",
        "            'missing_values_handled': bool(original_missing > 0),\n",
        "            'outliers_detected': int(sum([info['count'] for info in outlier_info.values()]) if 'outlier_info' in locals() else 0),\n",
        "            'class_balance_ratio': float(imbalance_ratio) if 'imbalance_ratio' in locals() else 1.0,\n",
        "            'balancing_applied': bool(needs_balancing) if 'needs_balancing' in locals() else False,\n",
        "            'processing_steps': [\n",
        "                'duplicate_removal',\n",
        "                'categorical_standardization',\n",
        "                'range_validation',\n",
        "                'missing_value_imputation',\n",
        "                'feature_engineering',\n",
        "                'class_balancing' if 'needs_balancing' in locals() and needs_balancing else 'no_balancing_needed'\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        import json\n",
        "        summary_path = os.path.join(processed_dir, \"data_preparation_summary.json\")\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(processing_summary, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Saved processing summary to: {summary_path}\")\n",
        "        \n",
        "        print(f\"\\nDATA PREPARATION SUMMARY:\")\n",
        "        print(f\"Target column: '{target_column}'\")\n",
        "        print(f\"Behavioral features: {len(behavioral_numeric_features)}\")\n",
        "        print(f\"Engineered features: {len(engineered_features)}\")\n",
        "        print(f\"Binary features: {len(binary_features_created)}\")\n",
        "        print(f\"Total modeling features: {len(all_features)}\")\n",
        "        \n",
        "        print(f\"\\nDatasets exported:\")\n",
        "        print(f\"- personality_dataset_cleaned.csv: Main processed dataset\")\n",
        "        if 'df_undersampled' in locals() and len(df_undersampled) != len(df):\n",
        "            print(f\"- personality_dataset_undersampled.csv: Balanced via undersampling\")\n",
        "        if 'df_oversampled' in locals() and len(df_oversampled) != len(df):\n",
        "            print(f\"- personality_dataset_oversampled.csv: Balanced via oversampling\")\n",
        "        if 'df_smote' in locals() and len(df_smote) != len(df):\n",
        "            print(f\"- personality_dataset_smote.csv: Balanced via SMOTE\")\n",
        "        print(f\"- feature_metadata.csv: Feature information for modeling\")\n",
        "        print(f\"- data_preparation_summary.json: Processing metadata\")\n",
        "        \n",
        "    else:\n",
        "        print(\"ERROR: Missing required target column for export\")\n",
        "        \n",
        "else:\n",
        "    print(f\"ERROR: Main data directory not found at {processed_dir}\")\n",
        "    print(\"Please ensure you're running from the notebooks/ folder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook successfully completed the data preparation phase for extrovert-introvert personality classification:\n",
        "\n",
        "**Step 1: Library Import**\n",
        "- Imported all required libraries for psychological data preprocessing\n",
        "- Configured statistical analysis and machine learning tools\n",
        "- Set up data imputation, scaling, and class balancing methods\n",
        "\n",
        "**Step 2: Data Loading**\n",
        "- Loaded personality dataset with robust encoding handling\n",
        "- Supported both processed and raw data sources\n",
        "- Validated data integrity and basic structure\n",
        "\n",
        "**Step 3: Data Validation and Feature Identification**\n",
        "- Automatically identified target and feature columns\n",
        "- Categorized behavioral vs psychological features\n",
        "- Performed comprehensive data quality assessment\n",
        "- Analyzed class balance and distribution\n",
        "\n",
        "**Step 4: Data Cleaning Functions**\n",
        "- Defined domain-specific missing value imputation strategies\n",
        "- Implemented outlier detection using IQR and Z-score methods\n",
        "- Created feature range validation for 0-10 behavioral scales\n",
        "- Built categorical feature standardization functions\n",
        "\n",
        "**Step 5: Data Cleaning Pipeline**\n",
        "- Removed duplicate records\n",
        "- Standardized categorical feature values\n",
        "- Validated and corrected feature ranges (0-10 scale)\n",
        "- Applied domain-specific missing value handling\n",
        "- Detected and analyzed outliers in behavioral data\n",
        "\n",
        "**Step 6: Feature Engineering**\n",
        "- Created composite personality scores (Social Activity, Introversion)\n",
        "- Engineered behavioral indicators (Social Comfort, Energy Balance)\n",
        "- Generated binary feature encodings for modeling\n",
        "- Built domain-specific features based on personality psychology\n",
        "\n",
        "**Step 7: Class Balancing**\n",
        "- Analyzed personality type distribution and imbalance\n",
        "- Applied multiple balancing techniques (undersampling, oversampling, SMOTE)\n",
        "- Created balanced datasets for different modeling approaches\n",
        "- Maintained data integrity across all transformations\n",
        "\n",
        "**Step 8: Data Export**\n",
        "- Exported cleaned datasets to main data directory\n",
        "- Created multiple dataset versions (original, undersampled, oversampled, SMOTE)\n",
        "- Generated comprehensive feature metadata for modeling\n",
        "- Saved processing summary with all transformation details\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to **Step 3: Data Exploration** for in-depth analysis\n",
        "- Use cleaned datasets for personality classification model training\n",
        "- Leverage engineered features for improved model performance\n",
        "- All processed data available in `data/processed/` directory\n",
        "\n",
        "### Key Outputs:\n",
        "- `personality_dataset_cleaned.csv`: Main processed dataset ready for modeling\n",
        "- `personality_dataset_undersampled.csv`: Balanced via undersampling (if needed)\n",
        "- `personality_dataset_oversampled.csv`: Balanced via oversampling (if needed)\n",
        "- `personality_dataset_smote.csv`: Balanced via SMOTE synthetic generation (if needed)\n",
        "- `feature_metadata.csv`: Detailed feature information for model development\n",
        "- `data_preparation_summary.json`: Complete processing pipeline documentation\n",
        "\n",
        "The dataset is now clean, feature-engineered, and ready for machine learning model development with multiple balancing options available based on modeling requirements.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
